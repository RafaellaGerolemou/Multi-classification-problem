{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf7Ph5_3cYbK"
   },
   "source": [
    "## Multi Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luq7sBtPcYLm"
   },
   "source": [
    "## Overview and Abstract\n",
    "\n",
    "- The aim of this notebook is to predict from the text of the document what industry the document is related to.\n",
    "\n",
    "- This notebook includes a multinomial Naive Bayes machine learning model, a dense deep neural network, a Convolutional neural network (CNN), a Long short-term memory (LSTM) neural network and a Gated Recurrent Unit (GRU) neural network to deal with the multi-label multi-classification task. \n",
    "\n",
    "- The Convolutional Neural Network outperforms the other neural networks with F1 score of 0.94."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMi5_Fgrb3Im"
   },
   "source": [
    "## Method\n",
    "- **Data processing:** Train and test dataset contains 13 and 11 attributes, respectively. The ‘label’ column is the target variable, which is based on the category column and exists only in the training dataset. In the label column there are 176 combinations, because there are different combinations from the 9 industries. We want our prediction to be based on the 9 labels with a singular industries and not based on the combinations. The contracts that belongs to a singular industry have one '1' in their corresponding label number. So, in order to remove those rows from our dataset, we dropped the lines with more that one '1'. After removing the rows, we lost almost 8K rows from the given dataset, which is not a problem, since our dataset contains around 98K rows (0.08% loss). Additionally, to complete the data processing, we concacated the two datasets as one. We dropped the attribute value, due to the number of null values. Also, the attribute category was removed, for the reason that is not presented in test set. The dataset has both categorical and text features. The categorical attributes of the dataset were encoded using the Label Encoding. For the text features of the dataset, we used some functions with the help of Natural Language Toolkit (NLTK) and spacy, in order to clean the text of the attributes. The text features attributes include the title, description and awarding authority columns, where the title has text from the English language, whether the other two columns contain text from the German language. The first step of the text preprocessing part is that we are creating the convert_string function which converts the text type into string and the capital letters into lower. Further, the second step is to create a function removing the string punctuations, the string digits and some other punctuation that were not in the English language. In addition, in order to remove the English stopwords, we used the remove_stopwards function, in order to remove words that do not provide valuable information for downstream. For stopwords function, we created two different functions for each language. Hence, with the help of the SnowballStemmer function we reduced the inflected (or sometimes derived) words to their word stem, base or root form (e.g., ‘walks’ and ‘walking’, converted into ‘walk’). For each language a separate function was created. Furthermore, the German language is very different from the English language, with unfamiliar symbols, punctuations and letters. In order to not miss anything of the aforementioned, we create the clean_text function, removing everything unnecessary from the two German attributes. After the preprocessing in order to feed and build the models, we created a new column ('text') in the preprocessed dataset, including all the data from the three text attributes into one.\n",
    " - Data processing code can be found in the following notebook:\n",
    "https://colab.research.google.com/drive/15ZSh7QNpZBmAUUBeFDw8BOYjyxsOXpaX?usp=sharing\n",
    "\n",
    "- **Multinomial Naive Bayes Baseline Machine Learning Model:** The analysis of categorical data, specifically text data, is one of the most common applications of machine learning. The most common Machine Learning classifiers for multi-label classification datasets are the Logistic Regression, the Random Forest and the multinomial Naïve Bayes. In the first attempt, we used Logistic Regression but the Colab crashes due to the Ram. Secondly, the Random Forest algorithm was taking too long to execute, since the dataset has almost 122K rows. Lastly, the most appropriate and successful algorithm based on the provided dataset, for the baseline Machine Learning model is the Multinomial Naïve Bayes. This algorithm is also well known for multi class prediction feature, with the ability to predict the probability of multiple classes of target variable. Also, Naïve Bayes classifiers are commonly used in text classification because they perform better in multiclass problems and have a higher success rate than other algorithms. As a consequence, it's popular in spam filtering and sentiment analysis. Especially, the Multinomial Naïve Bayes it's a classification method based on Bayes' Theorem and the presumption of predictor independence. The most important reason for using Naïve Bayes algorithm, is that its fast and easy to predict class of test dataset, especially in predictions of multi-classifications datasets. When the assumption of independence is met, a Naïve Bayes classifier outperforms other models such as logistic regression and needs less training data. Last but not least, for our dataset it performs better, since in comparison to numerical input variables, it performs well with categorical input variables. \n",
    "\n",
    "- **Dense Deep Neural Network:**\n",
    "For the Dense and Deep Neural Network, we use ‘tokenizer()’, to vectorize our text into a list of integers. Due to the large number of unique tokens, we select only a specific number of words, meaning that keeping only the most common 50K words. To deal with the different length of words for each text sequence, we use padding which simply pads the sequence of words with zeros. Three hidden layers are created that make use of the ReLu activation function and batch normalisation to minimise training time and improve the performance of the neural network Then we use dropout at a rate of 25% to randomly remove the 25% of the neurons from the previous layer. The 9 output layer neurons correspond to each class we want to predict. The SoftMax function is used in the output layer in order to assign probabilities to each class. An L2 kernel regularizer is implemented to avoid overfitting. We implement the widely used ‘sparse_categorical_crossentropy ()’ for the loss function and the optimiser ‘Adam’. We re-run the model after oversampling via SMOTE and because the performance has been improved we stick to the oversampling technique. \n",
    "\n",
    "- **Additional Neural Networks:**\n",
    "  - **KIM Convolutional Neural Network:**\n",
    "A Convolutional Neural Network, also known as CNN or ConvNet, is a class of neural networks that specializes in processing image data. However, CNN has performed well in a variety of text classification tasks. To feed the text data into the model, we have to represent them as an array of vectors (each word mapped to a specific vector in a vector space composed of the entire vocabulary). Using tokenizer, we vectorize our text into a list of integers. Due to the large number of unique tokens, we select only a specific number of words, meaning that keeping only the 10K most common words. To deal with the different length of words for each text sequence, we use padding which simply pads the sequence of words with zeros. The neural network architecture includes the following layers: \n",
    "    * Embedding layer which provides a dense representation of the tokens and their relative meanings. We specify the 3 arguments needed for this layer which are the size of the vocabulary in the training dataset, the size of the vector space in which words will be embedded and the length of the input sequences. For the vector dimension, based on experimentations with several numbers, we keep 20 dimensions. \n",
    "    * One-dimensional Convolutional Layer finds patterns in the sentences applying filters and generating feature maps, it is composed of 32 filters with a size of 3 and the ReLu function as activation function. Then, based on ReLu, we select the ‘He initializer’. \n",
    "    * Max pooling layer helps to subsample/select the most important features from the convolutional layer with pool size of 3 and stride equal to 1 in order to reduce the computational load, number of parameters and then minimizing the risk of overfitting. \n",
    "    * Dropout layer is used to improve the generalization of the model. The dropout rate is equal to 20% meaning that 20% of the neurons from the previous layer will be randomly removed. \n",
    "    * Same One-dimensional Convolutional Layer is produced with increasing number of 64 filters in order to capture more features in the dataset followed by same max pooling and dropout layer.  \n",
    "    * Final One-dimensional Convolutional Layer is produced with increasing number of 128 filters and followed by max pooling and dropout layer. \n",
    "    * Output Layer (dense) has 9 neurons that corresponds to each class and the SoftMax function to map a probability for each class. We use l2 kernel regularizer with a regularization factor of 0.01 \n",
    "\n",
    "    For loss function, we use the sparse categorical crossentropy widely used for multi-classification problems and adam optimizer. Finally, we re-run the model after oversampling via SMOTE and the performance is improved so we keep oversampling technique. \n",
    "\n",
    "  - **Long-Short Term Memory Neural Network:**\n",
    "Our second model is a simple Long-Short Term Memory Neural Network, in order to vectorize text we convert each text into a sequence.  First, we set the maximum number of words we want to use by limiting the dataset to the top 10000 words. The maximum number of words in each horizontal sequence is set to 250 as the model requires.  Then we use Tokenizer() to create unique tokens (105650 are found in our case). To create an LSTM model, we need the sequences to be of the same length. To do that we truncate and pad the input sequences and we get the shape of the data tensor. Then we convert the categorical labels into numbers to get a label tensor. Then, we proceed to split the data into train and test sets and to create the neural network layers.  \n",
    "    * The embedded layer is the first layer and represents vectors of 100 length for each word.  This layer is accompanied by a spatial 1D dropout layer that ignores neurons that are randomly selected during training, in order to prevent any decrease in the learning rate and overfitting. We use a 20% dropout which is a decent compromise to prevent overfitting while retaining the accuracy of the model. \n",
    "    * The second layer is the LSTM that has 100 memory units.  The dropout rate of the second layer is equal to 20% meaning that 20% of the neurons from the previous layer will be randomly removed. The recurrent dropout rate is also 20%, and drops the connections between the recurrent units  \n",
    "    * The output layer needs to create 9 output values since we have 9 classes. We use SoftMax as the activation function to make sure that each class is assigned decimal probabilities that all add up to 1. This is done in order to speed up training.  \n",
    "    * We use the Adam as our optimization algorithm for the network because is one of the most appropriate for noisy problems.  Kernel_regularizer is used to apply the default L2=0.01 regularization penalty on each layer’s kernel. The penalties will be added up in the loss function being optimized by the network. Categorical_crossentropy is used as the loss function because we have more than two classes.  Therefore, 5 epochs are trained while the batch size is the size of input being fed to the model and it is equal to 128. Finally, the callback function ‘Early Stopping’ is used to monitor training and stops it once the performance of the model stops improving. \n",
    "\n",
    "  - **Gated Reccurent Unit Neural Network:**\n",
    "For the Gated Recurrent Unit (GRU) network we follow a similar method as for the LSTM model, since we used the same embedding layer. Thus, the maximum words used for processing and the embedding dimension are 10000 and 100 respectively. We used two dropouts for the GRU layers with 30% spatial dropout and recurrent dropout. In the following two dense layers, we used 1024 neural networks with the use of ReLu activation function, and dropout 0.8 in order to reduce the overfitting. In the last layer we used 9 neurons, since we have 9 categories, by using SoftMax. The activation function ‘SoftMax’, is implemented just before the output layer in order to assign decimal probabilities to each class and allow the training to converge faster. The output layer uses the optimizer ‘Adam’, along with categorical_crossentropy as the loss function since this is a multi-class classification. Finally, the callback function ‘Early Stopping’ is used to monitor training and stops it once the performance of the model stops improving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D49GrSBi7wVX"
   },
   "source": [
    "# Results and Discussion\n",
    "- The performance of Multinomial Naive Bayes Baseline Machine Learning model is very low compared to the neural networks.\n",
    "- The best scores are derived from the additional neural networks, especially the Convolutional neural network that has the best score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O71Xt9Qc7t8O"
   },
   "source": [
    " Model | Macro-F1-score\n",
    "--- | --- \n",
    "Multinomial NB | 0.083\n",
    "Dense Deep NN | 0.45 \n",
    "Convolutional NN  | 0.94\n",
    "LSTM NN | 0.90\n",
    "GRU NN | 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ver2rM0b2_H"
   },
   "source": [
    "# Summary and Recommendation\n",
    "\n",
    "- Regarding the LSTM neural network, ideally, we are supposed to use class weights, by embedding a matrix which needs to be derived from global vectors for the word representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYiwSYFYb2wD"
   },
   "source": [
    "# References\n",
    "- Ray, S., 2017. 6 Easy steps to Learn Naive Bayes Algorithm with codes in Python and R. [online] Analytics Vidhya. Available at: <https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/>\n",
    "- Kaggle. 2019. Intro to Recurrent Neural Networks LST | GRU. [online] Available at: <https://www.kaggle.com/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru>\n",
    "- Brownlee, J., 2019. Difference Between Return Sequences and Return States for LSTMs in Keras. [online] Machine Learning Mastery. Available at: <https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/>\n",
    "- Chaubard, F. and Socher, R., 2019. Natural Language Processing with Deep Learning. p.CNNs (Convolutional Neural Networks) chapter\n",
    "- Kim, J., 2017. Understanding how Convolutional Neural Network (CNN) perform text classification with word embeddings. [online] Towards Data Science. Available at: <https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b>\n",
    "- Brownlee, J., 2018. How to Develop a Multichannel CNN Model for Text Classification. [online] Machine Learning Mastery. Available at: <https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/>\n",
    "- Kaggle. 2019. NLP with CNN. [online] Available at: <https://www.kaggle.com/adhamsuliman1993/nlp-with-cnn/code>\n",
    "- Kaggle. 2020. Text Classification using CNN. [online] Available at: <https://www.kaggle.com/au1206/text-classification-using-cnn>\n",
    "- GeeksforGeeks. 2020. An introduction to MultiLabel classification. [online] Available at: <https://www.geeksforgeeks.org/an-introduction-to-multilabel-classification/>\n",
    "- Artiwise. 2020. Multi-label Text Classification with Machine Learning and Deep Learning. [online] Available at: <https://medium.com/@artiwise_en/multi-label-text-classification-with-machine-learning-and-deep-learning-1a0565ee98c8>\n",
    "- Scikit-learn.org. n.d. Multiclass and Multioutput algorithms. [online] Available at: <https://scikit-learn.org/stable/modules/multiclass.html>\n",
    "- Nabi, J., 2018. Machine Learning - Text processing. [online] Towards Data Science. Available at: <https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958>\n",
    "- Machine Learning. 2017. Text Classification using Neural Networks. [online] Available at: <https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6>\n",
    "- Janakiev, N., n.d. Practical Text Classification With Python and Keras. [online] Real Python. Available at: <https://realpython.com/python-keras-text-classification/>\n",
    "- Shaikh, J., 2017. Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK.. [online] Towards Data Science. Available at: <https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a>\n",
    "- Kumar, S., 2019. Getting started with Text Preprocessing. [online] Kaggle. Available at: <https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing>\n",
    "- Chapter 4. Text Vectorization and transformation pipelines [online] Oreilly. Available at:<https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9drbPnrYb2n9"
   },
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "sb-4qfAxHTvt",
    "outputId": "626476f2-946d-4367-b391-76ba47af3e44"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eOz3ybGHYDu",
    "outputId": "e41cd49c-a483-4d3b-c754-2db9d77e3928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
      "CPU (s):\n",
      "2.904276865\n",
      "GPU (s):\n",
      "0.0386751899999922\n",
      "GPU speedup over CPU: 75x\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "sJtzN621J7Tz"
   },
   "outputs": [],
   "source": [
    "# Include your packages/imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import files\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, GRU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "# Add your models here\n",
    "\n",
    "# Add your functions for training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2jwK35iKHcO5"
   },
   "outputs": [],
   "source": [
    "#Read the full processed dataset\n",
    "df = pd.read_csv('dataset_task2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7nZWA9eHJ-BB"
   },
   "outputs": [],
   "source": [
    "#adding the text of the three columns into one\n",
    "df['text']= df['title']+' '+df['description']+' '+df['awarding_authority']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "5Fn3MiSzKAC0"
   },
   "outputs": [],
   "source": [
    "#splitting the dataset into train and test\n",
    "df_train = df[:90384]\n",
    "df_test = df[90384:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "BqIMAYxBLX47",
    "outputId": "4a27cf4c-d154-4857-fee8-88898e0e27de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>docid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>nature_of_contract</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>awarding_authority</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2493527426</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>germani wilhelmshaven clean servic</td>\n",
       "      <td>unterhalt glasrein</td>\n",
       "      <td>staatlich baumanag em wes</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>germani wilhelmshaven clean servic unterhalt g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2538215982</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>germani dresden engin design servic traffic in...</td>\n",
       "      <td>ab karlsruh stuttgart nurnberg leipzig dresd b...</td>\n",
       "      <td>db netz ag</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>germani dresden engin design servic traffic in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2204943443</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>germani germer heat ventil air condit instal work</td>\n",
       "      <td>fertigstell erst bauabschnitt erfolgt zweit ba...</td>\n",
       "      <td>gross kreisstadt germ</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>germani germer heat ventil air condit instal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2417769175</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>germani limbach board</td>\n",
       "      <td>einricht tafelsyst</td>\n",
       "      <td>gemeind limbach</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>germani limbach board einricht tafelsyst gemei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2242098706</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>germani frankfurt main landscap work green area</td>\n",
       "      <td>projekt neubau filial dortmund gewerk galabau ...</td>\n",
       "      <td>deutsch bundesbank beschaffungszentrum</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>germani frankfurt main landscap work green are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                               text\n",
       "0           0  ...  germani wilhelmshaven clean servic unterhalt g...\n",
       "1           1  ...  germani dresden engin design servic traffic in...\n",
       "2           2  ...  germani germer heat ventil air condit instal w...\n",
       "3           3  ...  germani limbach board einricht tafelsyst gemei...\n",
       "4           4  ...  germani frankfurt main landscap work green are...\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xQqkgy6vKCgB"
   },
   "outputs": [],
   "source": [
    "#removing the unexpected column: unnamed:0\n",
    "df = df.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "tNnCDmlXKDoD"
   },
   "outputs": [],
   "source": [
    "df_train.shape\n",
    "df_train = df_train.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nwU1JhKKKEpx"
   },
   "outputs": [],
   "source": [
    "#removing the decimal places\n",
    "df_train['label'] = df_train['label'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "IDwtqkQ5KFwx"
   },
   "outputs": [],
   "source": [
    "df_train['label'] = df_train['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9dXpfl7cKGzr"
   },
   "outputs": [],
   "source": [
    "df_train['label'] = df_train['label'].apply(str)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train['label'] = le.fit_transform(df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Kc9ww4JDKHrK"
   },
   "outputs": [],
   "source": [
    "df_test.shape\n",
    "df_test = df_test.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "f2tRhL34KJXD",
    "outputId": "29d6e937-fe9d-4937-c6a6-a2b46c7c1384"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>docid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>nature_of_contract</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>awarding_authority</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2493527426</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>germani wilhelmshaven clean servic</td>\n",
       "      <td>unterhalt glasrein</td>\n",
       "      <td>staatlich baumanag em wes</td>\n",
       "      <td>5</td>\n",
       "      <td>germani wilhelmshaven clean servic unterhalt g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2538215982</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>germani dresden engin design servic traffic in...</td>\n",
       "      <td>ab karlsruh stuttgart nurnberg leipzig dresd b...</td>\n",
       "      <td>db netz ag</td>\n",
       "      <td>3</td>\n",
       "      <td>germani dresden engin design servic traffic in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2204943443</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>germani germer heat ventil air condit instal work</td>\n",
       "      <td>fertigstell erst bauabschnitt erfolgt zweit ba...</td>\n",
       "      <td>gross kreisstadt germ</td>\n",
       "      <td>3</td>\n",
       "      <td>germani germer heat ventil air condit instal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2417769175</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>germani limbach board</td>\n",
       "      <td>einricht tafelsyst</td>\n",
       "      <td>gemeind limbach</td>\n",
       "      <td>8</td>\n",
       "      <td>germani limbach board einricht tafelsyst gemei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2242098706</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>germani frankfurt main landscap work green area</td>\n",
       "      <td>projekt neubau filial dortmund gewerk galabau ...</td>\n",
       "      <td>deutsch bundesbank beschaffungszentrum</td>\n",
       "      <td>3</td>\n",
       "      <td>germani frankfurt main landscap work green are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       docid  ...  label                                               text\n",
       "0      0  2493527426  ...      5  germani wilhelmshaven clean servic unterhalt g...\n",
       "1      1  2538215982  ...      3  germani dresden engin design servic traffic in...\n",
       "2      2  2204943443  ...      3  germani germer heat ventil air condit instal w...\n",
       "3      3  2417769175  ...      8  germani limbach board einricht tafelsyst gemei...\n",
       "4      4  2242098706  ...      3  germani frankfurt main landscap work green are...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-YfF6NiLBGA"
   },
   "source": [
    "# Multinomial Naive Bayes Baseline Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "whPNrGNYM8V1"
   },
   "outputs": [],
   "source": [
    "y = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "_QzxkfR3LEuy"
   },
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['index','label','title','description','awarding_authority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "MBsP-fuyMMYv",
    "outputId": "35707695-f3c0-4f31-d0e3-160f9e70f493"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>nature_of_contract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2493527426</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>germani wilhelmshaven clean servic unterhalt g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2538215982</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>germani dresden engin design servic traffic in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2204943443</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>germani germer heat ventil air condit instal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2417769175</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>germani limbach board einricht tafelsyst gemei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2242098706</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>germani frankfurt main landscap work green are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        docid  ...                                               text\n",
       "0  2493527426  ...  germani wilhelmshaven clean servic unterhalt g...\n",
       "1  2538215982  ...  germani dresden engin design servic traffic in...\n",
       "2  2204943443  ...  germani germer heat ventil air condit instal w...\n",
       "3  2417769175  ...  germani limbach board einricht tafelsyst gemei...\n",
       "4  2242098706  ...  germani frankfurt main landscap work green are...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "5rg5i0dqMTvn"
   },
   "outputs": [],
   "source": [
    "#trail1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "QWTXxfOLNWtD"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X['text'], y, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpHhMm8wNpl9"
   },
   "source": [
    "**TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "pofFRm_fNmCZ"
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=0.3, max_features=None, strip_accents='unicode', analyzer='word',ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "FG7DyplOOAMM"
   },
   "outputs": [],
   "source": [
    "X_train = (X_train).values.astype('U')\n",
    "X_val = (X_val).values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylmZgjgmNuYs",
    "outputId": "94aa11f1-f511-413e-a281-c1bcbe2591bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=0.3, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=1, stop_words=None, strip_accents='unicode',\n",
       "                sublinear_tf=1, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=1, vocabulary=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit(list(X_train) + list(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mymjCqPgNv61"
   },
   "outputs": [],
   "source": [
    "xtrain_tfv =  tfv.transform(X_train) \n",
    "xvalid_tfv = tfv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "jFutPuurOFOo"
   },
   "outputs": [],
   "source": [
    "#Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "\n",
    "y_pred = clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkQctdh0OFz8",
    "outputId": "d5c1aab2-6ccb-4ea3-9bab-52a4f98a7dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [ 1202   527  1245 10906  1027  1342   516   215  1097]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0]]\n",
      "0.08361989978799833\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       1.00      0.60      0.75     18077\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60     18077\n",
      "   macro avg       0.11      0.07      0.08     18077\n",
      "weighted avg       1.00      0.60      0.75     18077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred,y_val))\n",
    "print(f1_score(y_pred,y_val, average = 'macro'))\n",
    "print(classification_report(y_pred,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gugZlqAiOm5D"
   },
   "source": [
    "# Dense Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOtyL5e4OF4l",
    "outputId": "b0c6a139-e08d-4807-84e5-bd433da23acd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114965,)"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Km4Y3PerOF98"
   },
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(max_words)\n",
    "\n",
    "df['text'] = df['text'].apply(str)\n",
    "tokenizer.fit_on_texts(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "1_F7raqgOv6Y"
   },
   "outputs": [],
   "source": [
    "#sequences_to_matrix(sequences, mode='binary')- crashes due to RAM required \n",
    "sequence_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "CIX3gvHmO4z_"
   },
   "outputs": [],
   "source": [
    "sequence_test = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uezuzOgaO6Xs",
    "outputId": "c8544586-f603-4876-bc0a-4cb100c8939c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 109021 number of independent tokens\n"
     ]
    }
   ],
   "source": [
    "word_2_vec = tokenizer.word_index\n",
    "V = len(word_2_vec)\n",
    "\n",
    "print('Dataset has {} number of independent tokens'.format(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpbdYrk-O7jU",
    "outputId": "f493b3a4-85d2-4b65-cb11-fb1ce63b8e05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72307, 563)"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pad_sequences(sequence_train)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oICeVqkO9Av",
    "outputId": "73a3b364-d8c1-41ff-bf44-f6fe6093deb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18077, 563)"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = data_train.shape[1]\n",
    "\n",
    "data_test = pad_sequences(sequence_test, maxlen=T)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BX36KUz1QCrD",
    "outputId": "7e3b754f-f4a9-43d3-f5b1-5483221a4e1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_sample(data_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "kqahpBW6O-mD"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Dense(128,input_shape=(data_train[0].shape)))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(9, activation='softmax',kernel_regularizer=l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "fNsDYJOJPANh"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6METPY27PMJf",
    "outputId": "da214923-5ef3-40c5-d385-1c6f2fddec43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.7091 - accuracy: 0.7892 - val_loss: 1.5887 - val_accuracy: 0.6275\n",
      "Epoch 2/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6331 - accuracy: 0.8146 - val_loss: 1.5536 - val_accuracy: 0.6380\n",
      "Epoch 3/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.6108 - accuracy: 0.8217 - val_loss: 1.5521 - val_accuracy: 0.6405\n",
      "Epoch 4/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5982 - accuracy: 0.8265 - val_loss: 1.5770 - val_accuracy: 0.6413\n",
      "Epoch 5/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5842 - accuracy: 0.8302 - val_loss: 1.5430 - val_accuracy: 0.6524\n",
      "Epoch 6/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5808 - accuracy: 0.8313 - val_loss: 1.5516 - val_accuracy: 0.6425\n",
      "Epoch 7/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5709 - accuracy: 0.8330 - val_loss: 1.5939 - val_accuracy: 0.6475\n",
      "Epoch 8/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5658 - accuracy: 0.8352 - val_loss: 1.5731 - val_accuracy: 0.6463\n",
      "Epoch 9/75\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 0.5590 - accuracy: 0.8366 - val_loss: 1.5608 - val_accuracy: 0.6502\n",
      "Epoch 10/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5555 - accuracy: 0.8383 - val_loss: 1.5463 - val_accuracy: 0.6581\n",
      "Epoch 11/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5509 - accuracy: 0.8401 - val_loss: 1.5447 - val_accuracy: 0.6524\n",
      "Epoch 12/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5440 - accuracy: 0.8414 - val_loss: 1.5615 - val_accuracy: 0.6598\n",
      "Epoch 13/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5387 - accuracy: 0.8423 - val_loss: 1.6264 - val_accuracy: 0.6474\n",
      "Epoch 14/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5322 - accuracy: 0.8442 - val_loss: 1.6148 - val_accuracy: 0.6435\n",
      "Epoch 15/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5343 - accuracy: 0.8430 - val_loss: 1.5844 - val_accuracy: 0.6509\n",
      "Epoch 16/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5321 - accuracy: 0.8445 - val_loss: 1.5718 - val_accuracy: 0.6464\n",
      "Epoch 17/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5244 - accuracy: 0.8457 - val_loss: 1.6231 - val_accuracy: 0.6462\n",
      "Epoch 18/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5241 - accuracy: 0.8463 - val_loss: 1.6036 - val_accuracy: 0.6543\n",
      "Epoch 19/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5225 - accuracy: 0.8467 - val_loss: 1.6145 - val_accuracy: 0.6527\n",
      "Epoch 20/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5197 - accuracy: 0.8475 - val_loss: 1.5903 - val_accuracy: 0.6577\n",
      "Epoch 21/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5125 - accuracy: 0.8500 - val_loss: 1.6013 - val_accuracy: 0.6597\n",
      "Epoch 22/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5112 - accuracy: 0.8493 - val_loss: 1.5934 - val_accuracy: 0.6672\n",
      "Epoch 23/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5142 - accuracy: 0.8490 - val_loss: 1.6073 - val_accuracy: 0.6628\n",
      "Epoch 24/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5040 - accuracy: 0.8516 - val_loss: 1.6146 - val_accuracy: 0.6553\n",
      "Epoch 25/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5072 - accuracy: 0.8502 - val_loss: 1.6234 - val_accuracy: 0.6465\n",
      "Epoch 26/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5054 - accuracy: 0.8518 - val_loss: 1.5854 - val_accuracy: 0.6593\n",
      "Epoch 27/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.5003 - accuracy: 0.8539 - val_loss: 1.6242 - val_accuracy: 0.6571\n",
      "Epoch 28/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4965 - accuracy: 0.8541 - val_loss: 1.6515 - val_accuracy: 0.6639\n",
      "Epoch 29/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4954 - accuracy: 0.8550 - val_loss: 1.6381 - val_accuracy: 0.6570\n",
      "Epoch 30/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4906 - accuracy: 0.8555 - val_loss: 1.6568 - val_accuracy: 0.6676\n",
      "Epoch 31/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4953 - accuracy: 0.8544 - val_loss: 1.6312 - val_accuracy: 0.6607\n",
      "Epoch 32/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4897 - accuracy: 0.8558 - val_loss: 1.6110 - val_accuracy: 0.6690\n",
      "Epoch 33/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4865 - accuracy: 0.8566 - val_loss: 1.6193 - val_accuracy: 0.6704\n",
      "Epoch 34/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4856 - accuracy: 0.8569 - val_loss: 1.6260 - val_accuracy: 0.6607\n",
      "Epoch 35/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4844 - accuracy: 0.8577 - val_loss: 1.6339 - val_accuracy: 0.6539\n",
      "Epoch 36/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4819 - accuracy: 0.8579 - val_loss: 1.6363 - val_accuracy: 0.6642\n",
      "Epoch 37/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4781 - accuracy: 0.8589 - val_loss: 1.6522 - val_accuracy: 0.6638\n",
      "Epoch 38/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4793 - accuracy: 0.8585 - val_loss: 1.6397 - val_accuracy: 0.6614\n",
      "Epoch 39/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4766 - accuracy: 0.8590 - val_loss: 1.6301 - val_accuracy: 0.6636\n",
      "Epoch 40/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4778 - accuracy: 0.8594 - val_loss: 1.5924 - val_accuracy: 0.6686\n",
      "Epoch 41/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4692 - accuracy: 0.8624 - val_loss: 1.6657 - val_accuracy: 0.6685\n",
      "Epoch 42/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4697 - accuracy: 0.8617 - val_loss: 1.6961 - val_accuracy: 0.6663\n",
      "Epoch 43/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4726 - accuracy: 0.8604 - val_loss: 1.6438 - val_accuracy: 0.6639\n",
      "Epoch 44/75\n",
      "900/900 [==============================] - 3s 3ms/step - loss: 0.4691 - accuracy: 0.8620 - val_loss: 1.6540 - val_accuracy: 0.6722\n",
      "Epoch 45/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4631 - accuracy: 0.8633 - val_loss: 1.6931 - val_accuracy: 0.6689\n",
      "Epoch 46/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4619 - accuracy: 0.8647 - val_loss: 1.6752 - val_accuracy: 0.6710\n",
      "Epoch 47/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4643 - accuracy: 0.8630 - val_loss: 1.6846 - val_accuracy: 0.6742\n",
      "Epoch 48/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4585 - accuracy: 0.8648 - val_loss: 1.6556 - val_accuracy: 0.6708\n",
      "Epoch 49/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4593 - accuracy: 0.8648 - val_loss: 1.6834 - val_accuracy: 0.6766\n",
      "Epoch 50/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4571 - accuracy: 0.8647 - val_loss: 1.6747 - val_accuracy: 0.6672\n",
      "Epoch 51/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4636 - accuracy: 0.8635 - val_loss: 1.6585 - val_accuracy: 0.6664\n",
      "Epoch 52/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4593 - accuracy: 0.8639 - val_loss: 1.6965 - val_accuracy: 0.6709\n",
      "Epoch 53/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4529 - accuracy: 0.8659 - val_loss: 1.6934 - val_accuracy: 0.6686\n",
      "Epoch 54/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4512 - accuracy: 0.8669 - val_loss: 1.7037 - val_accuracy: 0.6723\n",
      "Epoch 55/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4506 - accuracy: 0.8667 - val_loss: 1.6714 - val_accuracy: 0.6720\n",
      "Epoch 56/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4455 - accuracy: 0.8690 - val_loss: 1.7185 - val_accuracy: 0.6791\n",
      "Epoch 57/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4516 - accuracy: 0.8670 - val_loss: 1.7011 - val_accuracy: 0.6764\n",
      "Epoch 58/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4492 - accuracy: 0.8675 - val_loss: 1.7143 - val_accuracy: 0.6781\n",
      "Epoch 59/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4466 - accuracy: 0.8681 - val_loss: 1.6952 - val_accuracy: 0.6751\n",
      "Epoch 60/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4459 - accuracy: 0.8679 - val_loss: 1.7199 - val_accuracy: 0.6744\n",
      "Epoch 61/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4419 - accuracy: 0.8697 - val_loss: 1.6766 - val_accuracy: 0.6733\n",
      "Epoch 62/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4414 - accuracy: 0.8691 - val_loss: 1.7178 - val_accuracy: 0.6751\n",
      "Epoch 63/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4418 - accuracy: 0.8702 - val_loss: 1.7027 - val_accuracy: 0.6763\n",
      "Epoch 64/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4394 - accuracy: 0.8703 - val_loss: 1.7625 - val_accuracy: 0.6758\n",
      "Epoch 65/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4413 - accuracy: 0.8701 - val_loss: 1.6929 - val_accuracy: 0.6736\n",
      "Epoch 66/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4401 - accuracy: 0.8706 - val_loss: 1.7375 - val_accuracy: 0.6797\n",
      "Epoch 67/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4378 - accuracy: 0.8707 - val_loss: 1.7124 - val_accuracy: 0.6704\n",
      "Epoch 68/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4378 - accuracy: 0.8702 - val_loss: 1.7635 - val_accuracy: 0.6731\n",
      "Epoch 69/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4359 - accuracy: 0.8718 - val_loss: 1.7610 - val_accuracy: 0.6685\n",
      "Epoch 70/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4365 - accuracy: 0.8706 - val_loss: 1.7417 - val_accuracy: 0.6731\n",
      "Epoch 71/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4309 - accuracy: 0.8734 - val_loss: 1.7428 - val_accuracy: 0.6824\n",
      "Epoch 72/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4286 - accuracy: 0.8733 - val_loss: 1.7294 - val_accuracy: 0.6774\n",
      "Epoch 73/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4302 - accuracy: 0.8735 - val_loss: 1.7706 - val_accuracy: 0.6778\n",
      "Epoch 74/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4293 - accuracy: 0.8735 - val_loss: 1.7808 - val_accuracy: 0.6762\n",
      "Epoch 75/75\n",
      "900/900 [==============================] - 3s 4ms/step - loss: 0.4313 - accuracy: 0.8720 - val_loss: 1.7790 - val_accuracy: 0.6754\n"
     ]
    }
   ],
   "source": [
    "nn = model.fit(X_sm, y_sm, validation_data=(data_test,y_val), batch_size=128, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLn9TPigPOdH",
    "outputId": "2044b2c1-1fa8-47f2-dd9d-e01a64c3f263"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5177588e-05, 2.2882006e-05, 2.6565665e-04, ..., 8.7775406e-06,\n",
       "        4.9202589e-07, 1.6864638e-05],\n",
       "       [5.6620096e-03, 1.2726370e-03, 1.0472906e-02, ..., 1.9940909e-04,\n",
       "        4.1122660e-03, 6.0702078e-03],\n",
       "       [2.1551408e-02, 3.3901431e-02, 3.4394044e-02, ..., 4.4942098e-03,\n",
       "        1.9417939e-04, 2.7550308e-02],\n",
       "       ...,\n",
       "       [1.8840066e-08, 6.2170458e-10, 2.8192071e-08, ..., 6.5559003e-09,\n",
       "        9.0935520e-16, 5.0390845e-06],\n",
       "       [4.0643636e-02, 3.1042235e-02, 3.3931188e-02, ..., 1.9132821e-02,\n",
       "        5.7891579e-03, 5.9540384e-02],\n",
       "       [2.7357565e-02, 3.0128075e-02, 2.8876664e-02, ..., 1.1134259e-03,\n",
       "        1.9256456e-03, 7.2576173e-02]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "za6yhbO0PQ28"
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8cQ5dlnPS5P",
    "outputId": "1477473e-3a93-42d4-db4b-5ea2e4ec5f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final=np.argmax(y_pred,axis=1)\n",
    "y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQRh5kLrP4UP",
    "outputId": "fe6bcab6-29f4-4379-9152-8953a5b63b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.30      0.37      1202\n",
      "           1       0.55      0.26      0.35       527\n",
      "           2       0.60      0.31      0.41      1245\n",
      "           3       0.74      0.89      0.81     10906\n",
      "           4       0.66      0.48      0.55      1027\n",
      "           5       0.50      0.30      0.37      1342\n",
      "           6       0.64      0.48      0.55       516\n",
      "           7       0.18      0.54      0.27       215\n",
      "           8       0.50      0.30      0.37      1097\n",
      "\n",
      "    accuracy                           0.68     18077\n",
      "   macro avg       0.54      0.43      0.45     18077\n",
      "weighted avg       0.66      0.68      0.65     18077\n",
      "\n",
      "0.451587741602686\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val,y_pred_final))\n",
    "print(f1_score(y_val,y_pred_final, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIhPHuy3RyV7",
    "outputId": "de493cd5-afd6-42d0-d317-3987ba540f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24581, 563)"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = data_train.shape[1]\n",
    "\n",
    "df_test['text'] = df_test['text'].apply(str)\n",
    "sequence_actual = tokenizer.texts_to_sequences(df_test['text'])\n",
    "data_actual = pad_sequences(sequence_actual, maxlen=T)\n",
    "data_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "QrgDQqFVR8AM"
   },
   "outputs": [],
   "source": [
    "y_actual = model.predict(data_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFCVVeZVR8LZ",
    "outputId": "12f8f732-2e54-4b8c-aa7d-69a360cd8802"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, ..., 3, 3, 7])"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual = np.argmax(y_actual, axis=1)\n",
    "y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLwd-aovR8U8",
    "outputId": "fa543c2d-3d4f-4188-b3fb-d1e3d063afaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24581,)"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "dyPVgc3HTrme",
    "outputId": "f67f952e-5a0e-40ec-dd29-538e633da73e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24576</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24577</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24578</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24579</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24580</th>\n",
       "      <td>010000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24581 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label\n",
       "0      000001000\n",
       "1      000001000\n",
       "2      000010000\n",
       "3      000001000\n",
       "4      000001000\n",
       "...          ...\n",
       "24576  000001000\n",
       "24577  000001000\n",
       "24578  000001000\n",
       "24579  000001000\n",
       "24580  010000000\n",
       "\n",
       "[24581 rows x 1 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual2 = le.inverse_transform(y_actual)\n",
    "\n",
    "y_actual2 = pd.DataFrame(y_actual2, columns=['label'])\n",
    "y_actual2['label'] = y_actual2['label'].apply(lambda x: x.zfill(9))\n",
    "y_actual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "plqDSEY8VS_O"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_actual2).set_index(df_test['docid']).rename(columns={0:'label'}).to_csv('NN_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY7pWxf6aGgK"
   },
   "source": [
    "# Additional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4dLY3AvWWDC"
   },
   "source": [
    "## 1. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "bUVQkybJVyN-"
   },
   "outputs": [],
   "source": [
    "max_words=10000\n",
    "\n",
    "tokenizer=Tokenizer(max_words)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequence_train=tokenizer.texts_to_sequences(X_train)\n",
    "sequence_test=tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DLU5_24WgJt",
    "outputId": "42be2d92-0492-4f5c-9eae-7c237a0098c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 109021 number of independent tokens\n"
     ]
    }
   ],
   "source": [
    "word_2_vec=tokenizer.word_index\n",
    "V=len(word_2_vec)\n",
    "\n",
    "print('Dataset has {} number of independent tokens'.format(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXKgmuibWh04",
    "outputId": "5ec509b4-f212-4e8e-9689-17ce0c0e352f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72307, 454)"
      ]
     },
     "execution_count": 143,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=pad_sequences(sequence_train)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOQ5K0O5Wjhl",
    "outputId": "ee8d6d27-af4f-40b2-d45b-d8f1687c5904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18077, 454)"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = data_train.shape[1]\n",
    "\n",
    "data_test = pad_sequences(sequence_test, maxlen = T)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZAHCokqWk3T",
    "outputId": "43eb3ca4-60d1-47a6-84ca-638d1dcdbb7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy = 'minority')\n",
    "X_sm, y_sm = smote.fit_sample(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ACmihnEWmfM",
    "outputId": "528eb024-344f-4aa3-fc25-4b8376ad67aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 454)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 454, 20)           2180440   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 452, 32)           1952      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 148, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 47, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 2,214,465\n",
      "Trainable params: 2,214,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = 20\n",
    "i = Input((T,))\n",
    "x = Embedding(V+1, D)(i)\n",
    "x = Conv1D(32, 3, kernel_initializer='he_uniform', activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = (Dropout(0.2))(x)\n",
    "x = Conv1D(64, 3, kernel_initializer='he_uniform', activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = (Dropout(0.2))(x)\n",
    "x = Conv1D(128, 3, kernel_initializer='he_uniform', activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = (Dropout(0.2))(x)\n",
    "x = Dense(9, activation='softmax', kernel_regularizer = l2(0.01))(x)\n",
    "model = Model(i, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "TERbT88AWomX"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLDQ3EkRWtSJ",
    "outputId": "a8c1f86f-02d5-47ed-fd9a-63be7c8c1131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.2297 - accuracy: 0.9355 - val_loss: 0.1543 - val_accuracy: 0.9582\n",
      "Epoch 2/15\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.1146 - accuracy: 0.9700 - val_loss: 0.1306 - val_accuracy: 0.9688\n",
      "Epoch 3/15\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.0874 - accuracy: 0.9779 - val_loss: 0.1182 - val_accuracy: 0.9717\n",
      "Epoch 4/15\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.0730 - accuracy: 0.9811 - val_loss: 0.1202 - val_accuracy: 0.9724\n",
      "Epoch 5/15\n",
      "900/900 [==============================] - 24s 26ms/step - loss: 0.0643 - accuracy: 0.9835 - val_loss: 0.1172 - val_accuracy: 0.9731\n",
      "Epoch 6/15\n",
      "900/900 [==============================] - 24s 26ms/step - loss: 0.0567 - accuracy: 0.9854 - val_loss: 0.1139 - val_accuracy: 0.9741\n",
      "Epoch 7/15\n",
      "900/900 [==============================] - 24s 26ms/step - loss: 0.0523 - accuracy: 0.9862 - val_loss: 0.1062 - val_accuracy: 0.9754\n",
      "Epoch 8/15\n",
      "900/900 [==============================] - 23s 26ms/step - loss: 0.0499 - accuracy: 0.9866 - val_loss: 0.1115 - val_accuracy: 0.9750\n",
      "Epoch 9/15\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.0452 - accuracy: 0.9879 - val_loss: 0.1088 - val_accuracy: 0.9746\n",
      "Epoch 10/15\n",
      "900/900 [==============================] - 24s 27ms/step - loss: 0.0420 - accuracy: 0.9886 - val_loss: 0.1183 - val_accuracy: 0.9743\n",
      "Epoch 11/15\n",
      "900/900 [==============================] - 24s 26ms/step - loss: 0.0416 - accuracy: 0.9887 - val_loss: 0.1086 - val_accuracy: 0.9763\n",
      "Epoch 12/15\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 0.0391 - accuracy: 0.9893 - val_loss: 0.1107 - val_accuracy: 0.9762\n",
      "Epoch 13/15\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 0.0368 - accuracy: 0.9897 - val_loss: 0.1132 - val_accuracy: 0.9761\n",
      "Epoch 14/15\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 0.0358 - accuracy: 0.9901 - val_loss: 0.1154 - val_accuracy: 0.9766\n",
      "Epoch 15/15\n",
      "900/900 [==============================] - 25s 28ms/step - loss: 0.0360 - accuracy: 0.9900 - val_loss: 0.1177 - val_accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "cnn = model.fit(X_sm, y_sm, validation_data=(data_test, y_val), batch_size=128, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wkPyyVuWyAH",
    "outputId": "ae8e016e-6f24-4439-dc83-6651d25a3341"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7424983e-11, 1.6726409e-16, 7.6045019e-14, ..., 8.1680433e-11,\n",
       "        6.7368694e-10, 1.9670496e-11],\n",
       "       [7.3270867e-21, 1.2751454e-13, 1.0000000e+00, ..., 5.1040306e-17,\n",
       "        7.0419924e-26, 1.2759795e-21],\n",
       "       [6.5742526e-08, 3.3495923e-12, 3.1139355e-10, ..., 5.0553957e-08,\n",
       "        3.2786907e-08, 6.0013129e-08],\n",
       "       ...,\n",
       "       [7.1521881e-12, 3.4724356e-18, 2.1706329e-11, ..., 1.4108348e-14,\n",
       "        7.0709310e-18, 1.4247949e-09],\n",
       "       [1.0631021e-12, 6.8982680e-23, 8.3002782e-19, ..., 3.0251588e-16,\n",
       "        7.4019099e-14, 3.1318399e-15],\n",
       "       [6.3762022e-09, 8.0994866e-15, 2.5312768e-12, ..., 2.2533728e-10,\n",
       "        1.1912837e-08, 7.1001782e-10]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "0DOwdxhbX1xJ"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmndXWlfX2A8",
    "outputId": "df5b96b8-b2dd-407e-e399-466328bd5d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, ..., 5, 3, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final=np.argmax(y_pred, axis=1)\n",
    "y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0lb7nfMX2NK",
    "outputId": "37f3ad85-3cea-47b6-bedf-945457e3b609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      1202\n",
      "           1       0.98      0.94      0.96       527\n",
      "           2       0.98      0.97      0.98      1245\n",
      "           3       0.98      1.00      0.99     10906\n",
      "           4       0.96      0.97      0.96      1027\n",
      "           5       0.98      0.95      0.97      1342\n",
      "           6       0.97      0.97      0.97       516\n",
      "           7       0.72      0.87      0.79       215\n",
      "           8       0.97      0.90      0.93      1097\n",
      "\n",
      "    accuracy                           0.98     18077\n",
      "   macro avg       0.95      0.94      0.95     18077\n",
      "weighted avg       0.98      0.98      0.98     18077\n",
      "\n",
      "0.9450903441794445\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred_final))\n",
    "print(f1_score(y_val, y_pred_final, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuuJU4kgZG-N",
    "outputId": "1dd31d19-bdba-4cbd-d8b7-4f0217f5d430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24581, 454)"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = X_sm.shape[1]\n",
    "\n",
    "sequence_actual = tokenizer.texts_to_sequences(df_test['text'])\n",
    "data_actual = pad_sequences(sequence_actual, maxlen=T)\n",
    "data_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "nvMNJtrxX2Vg"
   },
   "outputs": [],
   "source": [
    "y_actual_cnn = model.predict(data_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFtpPMhnX2gw",
    "outputId": "d1d3c04e-01e0-4906-cba4-186ea17615fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 7, ..., 3, 3, 5])"
      ]
     },
     "execution_count": 161,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual_cnn = np.argmax(y_actual_cnn, axis=1)\n",
    "y_actual_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "cbNcIu62X2r4",
    "outputId": "10c4ac2b-7a0c-4d31-9d99-176f9206965f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24576</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24577</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24578</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24579</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24580</th>\n",
       "      <td>000100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24581 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label\n",
       "0      000000100\n",
       "1      000001000\n",
       "2      010000000\n",
       "3      000001000\n",
       "4      000001000\n",
       "...          ...\n",
       "24576  000001000\n",
       "24577  000001000\n",
       "24578  000001000\n",
       "24579  000001000\n",
       "24580  000100000\n",
       "\n",
       "[24581 rows x 1 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual2 = le.inverse_transform(y_actual_cnn)\n",
    "\n",
    "y_actual2 = pd.DataFrame(y_actual2, columns=['label'])\n",
    "y_actual2['label'] = y_actual2['label'].apply(lambda x: x.zfill(9))\n",
    "y_actual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "ceiW_ne9Yhl-"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_actual2).set_index(df_test['docid']).rename(columns={0:'label'}).to_csv('CNN_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylRq0hpMZqls"
   },
   "source": [
    "## 2. Long Short-Term Memory Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLouWOzaYhyI",
    "outputId": "03fa8a70-77c5-4267-e9bb-8918d3c447e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109021 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Max wwords for processing \n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in a sequence\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "#fixed\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1xZhNr6Yh54",
    "outputId": "3d347d9c-ff4d-4757-c89b-65b8df6355cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (90384, 250)\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = df_train['text'].apply(str)\n",
    "X1 = tokenizer.texts_to_sequences(df_train['text'].values)\n",
    "X1 = pad_sequences(X1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46d1LA4paYWN",
    "outputId": "6a9b3682-e146-41c0-f992-5874822b4981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (90384, 9)\n"
     ]
    }
   ],
   "source": [
    "Y1 = pd.get_dummies(y).values\n",
    "\n",
    "print('Shape of label tensor:', Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdbTcTCWahrE",
    "outputId": "d9e7470b-09ba-4c41-866e-582908a5de5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72307, 250) (72307, 9)\n",
      "(18077, 250) (18077, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y1, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvjT3OtuaknU",
    "outputId": "cd670e11-64d1-4ac4-c4e0-3a88d2e7a8cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 1,081,309\n",
      "Trainable params: 1,081,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X1.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(9, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brMVrmc5amGj",
    "outputId": "72b64e3f-db9d-4fa4-8eec-04ea0b2da41a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "452/452 [==============================] - 372s 819ms/step - loss: 1.2099 - accuracy: 0.6675 - val_loss: 0.4949 - val_accuracy: 0.8696\n",
      "Epoch 2/5\n",
      "452/452 [==============================] - 366s 811ms/step - loss: 0.7858 - accuracy: 0.7924 - val_loss: 0.2871 - val_accuracy: 0.9468\n",
      "Epoch 3/5\n",
      "452/452 [==============================] - 364s 806ms/step - loss: 0.2542 - accuracy: 0.9537 - val_loss: 0.2560 - val_accuracy: 0.9521\n",
      "Epoch 4/5\n",
      "452/452 [==============================] - 363s 803ms/step - loss: 0.2185 - accuracy: 0.9614 - val_loss: 0.2155 - val_accuracy: 0.9660\n",
      "Epoch 5/5\n",
      "452/452 [==============================] - 366s 810ms/step - loss: 0.1804 - accuracy: 0.9728 - val_loss: 0.1991 - val_accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RGa4D2faoK2",
    "outputId": "730b05b9-b9a0-4258-b392-c5aa97cc01d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565/565 [==============================] - 32s 56ms/step - loss: 0.1928 - accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnW9B3l3arUQ",
    "outputId": "3884305e-cd5b-4543-b740-9cefeda7a3b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.5826574e-04, 1.4135386e-04, 3.7207917e-04, ..., 5.0347921e-04,\n",
       "        5.6711980e-04, 6.3459558e-04],\n",
       "       [9.4510510e-04, 3.7779568e-03, 9.7149110e-01, ..., 3.0791103e-03,\n",
       "        1.0260203e-03, 6.3530793e-03],\n",
       "       [5.5233983e-04, 1.9333945e-04, 4.4227808e-04, ..., 6.3904299e-04,\n",
       "        7.3623046e-04, 8.3656330e-04],\n",
       "       ...,\n",
       "       [6.8511521e-03, 1.0113606e-03, 4.2883945e-03, ..., 2.6862510e-03,\n",
       "        1.8898442e-03, 2.7827939e-03],\n",
       "       [4.7273119e-04, 1.5216056e-04, 3.6970977e-04, ..., 5.4716144e-04,\n",
       "        6.2427961e-04, 7.2582468e-04],\n",
       "       [6.8233197e-04, 2.8164641e-04, 7.0552021e-04, ..., 7.7690277e-04,\n",
       "        9.3062932e-04, 1.1100937e-03]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "9jnL6aRVaq_1"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkCMg0m3at9R",
    "outputId": "d9b92559-a13d-4a23-810a-ddea124b149f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, ..., 5, 3, 3])"
      ]
     },
     "execution_count": 176,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final = np.argmax(y_pred, axis=1)\n",
    "y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fiLmuIVavEK",
    "outputId": "02b2c174-98e3-4c3a-e3e5-70e6ab11fe59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1202\n",
      "           1       0.88      0.93      0.90       527\n",
      "           2       0.97      0.97      0.97      1245\n",
      "           3       1.00      0.99      0.99     10906\n",
      "           4       0.93      0.97      0.95      1027\n",
      "           5       0.95      0.97      0.96      1342\n",
      "           6       0.92      0.93      0.93       516\n",
      "           7       0.69      0.54      0.61       215\n",
      "           8       0.90      0.90      0.90      1097\n",
      "\n",
      "    accuracy                           0.97     18077\n",
      "   macro avg       0.91      0.91      0.91     18077\n",
      "weighted avg       0.97      0.97      0.97     18077\n",
      "\n",
      "0.9060204407029785\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred_final))\n",
    "print(f1_score(y_val, y_pred_final, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ix2Wm1D7azbs",
    "outputId": "10008d9f-2373-48e8-ca58-4a68d8d75eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (24581, 250)\n"
     ]
    }
   ],
   "source": [
    "act = tokenizer.texts_to_sequences(df_test['text'].values)\n",
    "act = pad_sequences(act, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "BXmU8vI2a68E"
   },
   "outputs": [],
   "source": [
    "y_actual_lstm = model.predict(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "forbvDaUa8rD",
    "outputId": "70d6639b-63c5-4266-8afc-c54c1f2e1341"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 7, ..., 3, 3, 5])"
      ]
     },
     "execution_count": 189,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual_lstm = np.argmax(y_actual_lstm, axis=1)\n",
    "y_actual_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-82V5oombKtq",
    "outputId": "d13dc56d-7196-4850-8caa-ba8a97be0a21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24576</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24577</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24578</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24579</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24580</th>\n",
       "      <td>000100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24581 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label\n",
       "0      000000100\n",
       "1      000001000\n",
       "2      010000000\n",
       "3      000001000\n",
       "4      000001000\n",
       "...          ...\n",
       "24576  000001000\n",
       "24577  000001000\n",
       "24578  000001000\n",
       "24579  000001000\n",
       "24580  000100000\n",
       "\n",
       "[24581 rows x 1 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual2 = le.inverse_transform(y_actual_lstm)\n",
    "\n",
    "y_actual2 = pd.DataFrame(y_actual2, columns=['label'])\n",
    "y_actual2['label'] = y_actual2['label'].apply(lambda x: x.zfill(9))\n",
    "y_actual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "MyPwfI3ubO_E"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_actual2).set_index(df_test['docid']).rename(columns={0:'label'}).to_csv('LSTM_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLOt6WIebU2T"
   },
   "source": [
    "## 3. Gated Recurrent Unit Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "xvZ1vB06bamz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X1,Y1, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zg0b3wJTbf8Z",
    "outputId": "28a26cac-c490-40d2-cd50-9aa9ed7bec9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "452/452 [==============================] - 708s 2s/step - loss: 1.3819 - val_loss: 0.9485\n",
      "Epoch 2/5\n",
      "452/452 [==============================] - 709s 2s/step - loss: 0.9906 - val_loss: 0.7397\n",
      "Epoch 3/5\n",
      "452/452 [==============================] - 706s 2s/step - loss: 0.8145 - val_loss: 0.5596\n",
      "Epoch 4/5\n",
      "452/452 [==============================] - 712s 2s/step - loss: 0.6579 - val_loss: 0.4628\n",
      "Epoch 5/5\n",
      "452/452 [==============================] - 708s 2s/step - loss: 0.5768 - val_loss: 0.3731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc39cb6c050>"
      ]
     },
     "execution_count": 200,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X1.shape[1],trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "h9aSfzSR1523"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOB3NF9tnYAy",
    "outputId": "6c8a08bd-ff1a-4d9f-db82-1f3cba89f62e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, ..., 5, 3, 3])"
      ]
     },
     "execution_count": 204,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwm3BDv3bgWN",
    "outputId": "a8203820-b09a-45ad-8e63-314a9fd47022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      1202\n",
      "           1       0.94      0.46      0.62       527\n",
      "           2       0.68      0.94      0.79      1245\n",
      "           3       0.99      0.96      0.97     10906\n",
      "           4       0.96      0.60      0.74      1027\n",
      "           5       0.91      0.78      0.84      1342\n",
      "           6       0.90      0.79      0.84       516\n",
      "           7       0.00      0.00      0.00       215\n",
      "           8       0.46      0.87      0.60      1097\n",
      "\n",
      "    accuracy                           0.88     18077\n",
      "   macro avg       0.74      0.69      0.70     18077\n",
      "weighted avg       0.90      0.88      0.88     18077\n",
      "\n",
      "0.6957060612979331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred_final))\n",
    "print(f1_score(y_val, y_pred_final, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjMfoYDd2R_a",
    "outputId": "558d7a13-f081-4620-9ed7-5fbd941ae698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (24581, 250)\n"
     ]
    }
   ],
   "source": [
    "act = tokenizer.texts_to_sequences(df_test['text'].values)\n",
    "act = pad_sequences(act, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "bXtYzi392ct0"
   },
   "outputs": [],
   "source": [
    "y_actual_gru = model.predict(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nJBEvSp2hZW",
    "outputId": "6f3948b5-035e-481c-8307-af1df4080913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 8, ..., 3, 3, 5])"
      ]
     },
     "execution_count": 208,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual_gru = np.argmax(y_actual_gru, axis=1)\n",
    "y_actual_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ryMc0LF_2lrN",
    "outputId": "3a5dfd16-0b73-4c4e-b4ad-028efee5e357"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24576</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24577</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24578</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24579</th>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24580</th>\n",
       "      <td>000100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24581 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label\n",
       "0      000000100\n",
       "1      000001000\n",
       "2      100000000\n",
       "3      000001000\n",
       "4      000001000\n",
       "...          ...\n",
       "24576  000001000\n",
       "24577  000001000\n",
       "24578  000001000\n",
       "24579  000001000\n",
       "24580  000100000\n",
       "\n",
       "[24581 rows x 1 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual2 = le.inverse_transform(y_actual_gru)\n",
    "\n",
    "y_actual2 = pd.DataFrame(y_actual2, columns=['label'])\n",
    "y_actual2['label'] = y_actual2['label'].apply(lambda x: x.zfill(9))\n",
    "y_actual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "LL3VT7LD2px7"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_actual2).set_index(df_test['docid']).rename(columns={0:'label'}).to_csv('GRU_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze-JOWC82uDk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1-YfF6NiLBGA",
    "ylRq0hpMZqls"
   ],
   "name": "Multi_Classification_Problem_FINAL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
